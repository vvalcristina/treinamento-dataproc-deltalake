{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conhecendo o DeltaLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instalando as dependencias\n",
    "!pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.appName(\"JobDeltaLake\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:0.8.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.databricks.delta.schema.autoMerge.enabled\",\"true\") \\\n",
    "    .config(\"spark.databricks.delta.autoOptimize.optimizeWrite\",\"true\") \\\n",
    "    .config(\"spark.databricks.delta.optimizeWrite.enabled\",\"true\") \\\n",
    "    .config(\"spark.databricks.delta.vacuum.parallelDelete.enabled\",\"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.172:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>JobDeltaLake</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5de4f04c90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando os paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_input = 'input/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_output = 'output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf input/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando os datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+\n",
      "| id|      data|               texto|\n",
      "+---+----------+--------------------+\n",
      "|  1|2021-05-30|Primeira linha da...|\n",
      "|  2|2021-05-30|Segunda linha dat...|\n",
      "+---+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1 = spark.createDataFrame(\n",
    "    [\n",
    "       (1, date.today(), 'Primeira linha dataframe'),\n",
    "       (2, date.today(), 'Segunda linha dataframe'),\n",
    "    ],\n",
    "        ['id','data', 'texto']\n",
    ")\n",
    "\n",
    "df_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos salvar o arquivo em formato parquet, para emular a leitura de um bucket no Storage com arquivos .parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Escrevendo o arquivo no formato parquet\n",
    "df_1.write.format(\"parquet\").save(path_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------------------+\n",
      "|id |data      |texto                   |\n",
      "+---+----------+------------------------+\n",
      "|1  |2021-05-30|Primeira linha dataframe|\n",
      "|2  |2021-05-30|Segunda linha dataframe |\n",
      "+---+----------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fazendo a leitura\n",
    "df_1 = spark.read.format(\"parquet\").option(\"inferSchema\", \"false\").load(path_input)\n",
    "df_1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Escrevendo o dataframe em formato delta\n",
    "df_1.write.format(\"delta\").save(path_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------------------+\n",
      "|id |data      |texto                   |\n",
      "+---+----------+------------------------+\n",
      "|1  |2021-05-30|Primeira linha dataframe|\n",
      "|2  |2021-05-30|Segunda linha dataframe |\n",
      "+---+----------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Verificando se o arquivo foi salvo no formato delta\n",
    "df = spark.read.format(\"delta\").load(path_output)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------------------------+\n",
      "|id |data      |texto                      |\n",
      "+---+----------+---------------------------+\n",
      "|1  |2021-05-30|Primeira linha dataframe   |\n",
      "|2  |2021-05-30|Segunda linha att dataframe|\n",
      "|3  |2021-05-30|Terceira linha dataframe   |\n",
      "+---+----------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Vamos criar um segundo dataframe\n",
    "df_2 = spark.createDataFrame(\n",
    "    [\n",
    "        (1, date.today(), 'Primeira linha dataframe'),\n",
    "        (2, date.today(), 'Segunda linha att dataframe'),\n",
    "        (3, date.today(), 'Terceira linha dataframe'),\n",
    "\n",
    "    ],\n",
    "        ['id','data', 'texto']\n",
    ")\n",
    "\n",
    "df_2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Escrevendo em tabela\n",
    "table = DeltaTable.forPath(spark, path_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------------------+\n",
      "|id |data      |texto                   |\n",
      "+---+----------+------------------------+\n",
      "|1  |2021-05-30|Primeira linha dataframe|\n",
      "|2  |2021-05-30|Segunda linha dataframe |\n",
      "+---+----------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table.toDF().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fazendo o merge dos arquivos\n",
    "table.alias(\"persisteddata\") .merge( \\\n",
    "   df_2.alias(\"newdata\"), \\\n",
    "    \"persisteddata.id = newdata.id\") \\\n",
    ".whenMatchedUpdateAll() \\\n",
    ".whenNotMatchedInsertAll() \\\n",
    ".execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------------------------+\n",
      "|id |data      |texto                      |\n",
      "+---+----------+---------------------------+\n",
      "|2  |2021-05-30|Segunda linha att dataframe|\n",
      "|3  |2021-05-30|Terceira linha dataframe   |\n",
      "|1  |2021-05-30|Primeira linha dataframe   |\n",
      "+---+----------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table.toDF().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.vacuum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- data: date (nullable = true)\n",
      " |-- texto: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Verificando o Schema\n",
    "table.toDF().printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificando as vers√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(path_output)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(path_output)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
