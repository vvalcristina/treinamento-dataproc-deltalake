jobs:
- pysparkJob:
    args:
    - deltalake
    #Caminho do job de execução no GCS
    mainPythonFileUri: gs://{{GCP_BUCKET_CODE_NAME}}/jobs/job_deltalake.py
  stepId: deltalake
placement:
  managedCluster:
    clusterName: deltalake
    config:
      gceClusterConfig:
        networkUri: 
        privateIpv6GoogleAccess: OUTBOUND
        #Permissões necessárias para execução desse template
        serviceAccountScopes:
          - https://www.googleapis.com/auth/bigquery
          - https://www.googleapis.com/auth/cloud.useraccounts.readonly
          - https://www.googleapis.com/auth/devstorage.full_control
          - https://www.googleapis.com/auth/devstorage.read_write
          - https://www.googleapis.com/auth/logging.write
        zoneUri: {{ZONE}}
        metadata: 
          bucket: {{GCP_CODE_BUCKET_NAME}}
      #Ações a serem rodadas antes do inicio do Job
      initializationActions:
        - executableFile: gs://{{GCP_BUCKET_CODE_NAME}}/scripts/init_actions
      #Configurações das máquinas do Cluster Dataproc
      masterConfig:
        diskConfig:
          bootDiskSizeGb: 500
          bootDiskType: pd-standard
        machineTypeUri: n1-standard-4
        minCpuPlatform: AUTOMATIC
        numInstances: 1
      workerConfig:
        diskConfig:
          bootDiskSizeGb: 500
          bootDiskType: pd-standard
        machineTypeUri: n1-standard-4
        minCpuPlatform: AUTOMATIC
        numInstances: 2
      #Nome e versão da imagem do cluster e propriedades necessarias para a execução de job com deltalake
      softwareConfig:
        imageVersion: 2.0.0-RC22-ubuntu18
        properties:
          spark:spark.jars.packages: 'io.delta:delta-core_2.12:0.8.0,com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.19.1'
          spark:spark.sql.extensions: io.delta.sql.DeltaSparkSessionExtension
          spark:spark.sql.catalog.spark_catalog: org.apache.spark.sql.delta.catalog.DeltaCatalog
          spark:spark.databricks.delta.schema.autoMerge.enabled: "true"
